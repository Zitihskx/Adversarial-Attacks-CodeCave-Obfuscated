{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fed841e-6439-4b11-bde1-8d0f409b8c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44689/231506103.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  malconv = torch.load('/home/user/Desktop/Retraining_Malconv/checkpoint/Retrain_all_samples_sd_850.model', map_location=torch.device('cuda') if use_gpu else 'cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MalConv model successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.util import ExeDataset, write_pred\n",
    "from src.model import MalConv\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "use_gpu = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "malconv = torch.load('/home/user/Desktop/Retraining_Malconv/checkpoint/Retrain_all_samples_sd_850.model', map_location=torch.device('cuda') if use_gpu else 'cpu')\n",
    "malconv = malconv.cuda() if use_gpu else malconv\n",
    "bce_loss = nn.BCEWithLogitsLoss().cuda() if use_gpu else nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Loading MalConv model successful\")\n",
    "\n",
    "data_path = '/home/user/Desktop/CodeCaveFinal-main/KkrunchyCodeCave/Cave12288_kkrunchy2/'\n",
    "csv_path='/home/user/Desktop/CodeCaveFinal-main/KkrunchyCodeCave/Cave12288_kkrunchy2_caves.csv'\n",
    "\n",
    "validloader = DataLoader(ExeDataset(data_path, csv_path),\n",
    "                         batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4529a39-2553-403a-8431-a8d219eb6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function_gradient(malconv, original_x, adv_x, use_cuda=False):\n",
    "    y = malconv.embedd_and_forward(adv_x)\n",
    "    y = nn.Sigmoid()(y)\n",
    "    \n",
    "    malware_class = torch.ones(y.shape)\n",
    "    if use_cuda:\n",
    "        malware_class = malware_class.cuda()\n",
    "    loss = torch.nn.functional.binary_cross_entropy(y, malware_class)\n",
    "    g = torch.autograd.grad(loss, adv_x)[0]\n",
    "    g = torch.transpose(g, -1, -2)[0]\n",
    "    return g\t\n",
    "\n",
    "def optimization_solver(gradient_f, index_to_consider, x_init):\n",
    "    device = gradient_f.device  # Get the device of gradient_f (it should be on the same device as your model)\n",
    "\n",
    "    # Create a zero tensor on the same device as gradient_f\n",
    "    zero_tensor = torch.zeros(gradient_f.shape, device=device)\n",
    "\n",
    "    # Compare if gradient_f is zero using tensors on the same device\n",
    "    g = gradient_f / torch.norm(gradient_f) if not torch.equal(gradient_f, zero_tensor) else torch.zeros(gradient_f.shape, device=device)\n",
    "\n",
    "    epsilon = 100\n",
    "    gradient_result = (epsilon * g).transpose(0,1)\n",
    "    x_init_updated = x_init.clone()  # Create a clone of x_init to avoid in-place operation\n",
    "    x_init_updated[0, :, index_to_consider] = x_init_updated[0, :, index_to_consider] + gradient_result[:, index_to_consider]\n",
    "    \n",
    "    return x_init_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c57414-16d3-4216-9b0a-44a3922e703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "mp.set_start_method('spawn', force=True)\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca40b9b-b0f5-468c-b424-9ab237a49557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 0\n",
      "0\n",
      "Completed: 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44689/3842646528.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_data_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008701976\n",
      "0.0017551557\n",
      "0.32658058\n",
      "0.0032167856\n",
      "0.119298734\n",
      "0.015874343\n",
      "0.04343372\n",
      "0.015398373\n",
      "0.06984804\n",
      "0.16930638\n",
      "0.028546214\n",
      "0.38856864\n",
      "0.23490058\n",
      "0.37924513\n",
      "0.018499786\n",
      "0.00096604903\n",
      "0.12549923\n",
      "0.35987362\n",
      "0.06953656\n",
      "0.031094015\n",
      "0.06772896\n",
      "0.058493603\n",
      "Completed: 100\n",
      "22\n",
      "0.26274627\n",
      "0.03389103\n",
      "0.030542253\n",
      "0.13597514\n",
      "0.16407913\n",
      "0.00035944168\n",
      "0.27381164\n",
      "0.18637933\n",
      "0.0011975808\n",
      "0.15452793\n",
      "0.06671814\n",
      "1.1858838e-05\n",
      "0.012753675\n",
      "0.0074908338\n",
      "0.2925698\n",
      "0.15679964\n",
      "0.031405404\n",
      "0.011894156\n",
      "0.26135993\n",
      "0.00013088506\n",
      "0.21263862\n",
      "8.8217115e-07\n",
      "0.0002649043\n",
      "0.24680625\n",
      "4.908064e-11\n",
      "Completed: 200\n",
      "47\n",
      "0.002790879\n",
      "0.012998398\n",
      "0.1719878\n",
      "0.4412909\n",
      "0.16798134\n",
      "0.2368184\n",
      "0.13782193\n",
      "0.32363725\n",
      "0.4976642\n",
      "0.0015222302\n",
      "3.3499875e-17\n",
      "0.235999\n",
      "0.14903829\n",
      "0.01072855\n",
      "0.028648995\n",
      "0.017892106\n",
      "0.0050012544\n",
      "0.017042667\n",
      "0.07300224\n",
      "0.15966989\n",
      "0.26532453\n",
      "0.04011331\n",
      "0.008416694\n",
      "0.14355448\n",
      "0.049790364\n",
      "0.00077245577\n",
      "0.2329\n",
      "0.1553534\n",
      "0.14434016\n",
      "0.14888622\n",
      "Completed: 300\n",
      "77\n",
      "0.004130244\n",
      "0.3672583\n",
      "0.35196868\n",
      "0.39874837\n",
      "0.0010116585\n",
      "0.016977731\n",
      "0.00089139934\n",
      "0.0861301\n",
      "0.27465817\n",
      "0.026124254\n",
      "0.08807596\n",
      "0.035381146\n",
      "0.010596744\n",
      "0.43611702\n",
      "0.016473243\n",
      "0.008054933\n",
      "0.05666994\n",
      "0.046919808\n",
      "0.368265\n",
      "0.39657336\n",
      "0.017758776\n",
      "0.46509048\n",
      "0.011011903\n",
      "0.44509503\n",
      "Completed: 400\n",
      "101\n",
      "0.42894593\n",
      "0.15629312\n",
      "0.0007169621\n",
      "0.02879598\n",
      "0.33564255\n",
      "0.21924275\n",
      "0.34987563\n",
      "0.2756548\n",
      "0.33572406\n",
      "0.12242445\n",
      "0.016906638\n",
      "0.004171494\n",
      "0.15870969\n",
      "0.1903009\n",
      "0.09874501\n",
      "4.6120253e-08\n",
      "0.46930054\n",
      "0.046292122\n",
      "0.43466637\n",
      "0.011906279\n",
      "0.022682508\n",
      "0.009307013\n",
      "0.06895149\n",
      "0.00043525922\n",
      "0.00032417366\n",
      "2.1911004e-07\n",
      "Completed: 500\n",
      "127\n",
      "0.029758072\n",
      "0.013971474\n",
      "0.09751352\n",
      "0.24943528\n",
      "0.0008971701\n",
      "0.18589808\n",
      "0.09748882\n",
      "0.42484426\n",
      "0.018644018\n",
      "0.26531115\n",
      "0.012908072\n",
      "0.007899215\n",
      "0.29800782\n",
      "0.005155427\n",
      "0.0059631146\n",
      "0.024723995\n",
      "0.019817932\n",
      "0.3614789\n",
      "0.012834437\n",
      "0.09313756\n",
      "0.47314125\n",
      "0.26477227\n",
      "1.2054921e-11\n",
      "0.39119363\n",
      "0.037154194\n",
      "0.024991512\n",
      "0.10725475\n",
      "Completed: 600\n",
      "154\n",
      "0.006613676\n",
      "0.12373504\n",
      "0.008630871\n",
      "0.0008384183\n",
      "0.17979783\n",
      "0.002111564\n",
      "0.008000178\n",
      "0.4756328\n",
      "0.21671595\n",
      "0.35302243\n",
      "0.017729321\n",
      "0.0023401047\n",
      "0.0032449656\n",
      "3.079496e-10\n",
      "0.4205003\n",
      "0.22500499\n",
      "0.1463547\n",
      "0.108383216\n",
      "0.43183553\n",
      "0.11842964\n",
      "0.21615206\n",
      "Completed: 700\n",
      "175\n",
      "0.2571897\n",
      "0.15868254\n",
      "0.012413493\n",
      "0.022697631\n",
      "0.3149858\n",
      "0.04190181\n",
      "0.0012817035\n",
      "0.040965773\n",
      "0.21574916\n",
      "0.35824513\n",
      "2.8528502e-08\n",
      "0.3318477\n",
      "0.028859828\n",
      "0.3842814\n",
      "0.33095124\n",
      "0.40876704\n",
      "0.008842944\n",
      "1.6590255e-05\n",
      "3.666886e-08\n",
      "0.23079419\n",
      "4.4830114e-05\n",
      "0.003104504\n",
      "0.39607668\n",
      "0.0043849316\n",
      "Completed: 800\n",
      "199\n",
      "Completed: 800\n",
      "199\n",
      "0.1757003\n",
      "0.11704919\n",
      "0.06977238\n",
      "0.2150368\n",
      "0.4530128\n",
      "0.049617033\n",
      "0.06412483\n",
      "0.004561173\n",
      "0.13442132\n",
      "0.018436402\n",
      "0.015625216\n",
      "0.15523586\n",
      "0.042373847\n",
      "8.2582876e-05\n",
      "0.0019091243\n",
      "0.00549155\n",
      "0.010275627\n",
      "0.34221783\n",
      "0.002799055\n",
      "0.21269064\n",
      "0.004461288\n",
      "0.37457845\n",
      "0.16057338\n",
      "0.06344693\n",
      "0.31495085\n",
      "0.0055381316\n",
      "Completed: 900\n",
      "225\n",
      "0.00014031652\n",
      "0.032639466\n",
      "0.38293228\n",
      "0.31319994\n",
      "0.4495865\n",
      "0.0025137053\n",
      "0.2578182\n",
      "0.3728394\n",
      "0.4330426\n",
      "0.014696438\n",
      "0.03525263\n",
      "0.0020465858\n",
      "0.0030960478\n",
      "0.00017274014\n",
      "2.042676e-06\n",
      "0.016799362\n",
      "0.3080083\n",
      "0.11223732\n",
      "0.44194576\n",
      "0.18095852\n",
      "0.056764275\n",
      "0.31436998\n",
      "Completed: 1000\n",
      "247\n",
      "0.08269432\n",
      "0.44099948\n",
      "0.0004143556\n",
      "0.047956213\n",
      "0.042527106\n",
      "0.2529791\n",
      "0.004772957\n",
      "0.47433355\n",
      "0.13601546\n",
      "0.15481903\n",
      "0.03450665\n",
      "0.08408968\n",
      "0.00010257463\n",
      "0.0004588801\n",
      "0.037541553\n",
      "4.2985263e-05\n",
      "Completed: 1100\n",
      "263\n",
      "0.008523818\n",
      "0.47287208\n",
      "0.028731985\n",
      "0.30761123\n",
      "0.14428149\n",
      "0.0047017606\n",
      "0.01604296\n",
      "0.27715534\n",
      "0.07342882\n",
      "0.00038605146\n",
      "3.7878562e-19\n",
      "0.0028584523\n",
      "0.1852199\n",
      "0.122858725\n",
      "0.000679406\n",
      "0.0055451347\n",
      "0.0023534903\n",
      "0.058970556\n",
      "0.00044823205\n",
      "0.07853604\n",
      "0.008640942\n",
      "0.4859075\n",
      "Completed: 1200\n",
      "285\n",
      "0.16647795\n",
      "0.19647667\n",
      "0.03317988\n",
      "0.27188048\n",
      "0.16609998\n",
      "0.311307\n",
      "0.45418644\n",
      "0.09687587\n",
      "0.45484042\n",
      "0.013552794\n",
      "0.010395734\n",
      "0.07451586\n",
      "0.13594383\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['val_batch_data', 'length', 'init_prob','iteration','progress'])\n",
    "counter = 0\n",
    "success=0\n",
    "for _, val_batch_data in enumerate(validloader):\n",
    "    if counter%100==0:\n",
    "        print(f\"Completed: {counter}\")\n",
    "        print(success)\n",
    "        # df.to_csv('Malconv_UPXPack_first16384Bytes.csv', mode='a', header=False, index=False) \n",
    "        # df = pd.DataFrame(columns=['val_batch_data', 'length', 'init_prob','iteration','progress'])\n",
    "        # torch.cuda.empty_cache()\n",
    "    exe_input = val_batch_data[0].cuda() if use_gpu else val_batch_data[0]\n",
    "    data = exe_input[0].cpu().numpy()\n",
    "    length = data[-3]\n",
    "    cave_start = data[-2]\n",
    "    cave_length = data[-1]\n",
    "\n",
    "    if cave_length==0:\n",
    "        continue\n",
    "    \n",
    "    data = data[:length]\n",
    "    data = np.concatenate([data, np.random.randint(0, 256, 2000000 - length)])\n",
    "\n",
    "    embed = malconv.embed\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    x0 = torch.from_numpy(np.array([data])).long().cuda() if use_gpu else torch.from_numpy(np.array([data])).long()\n",
    "    x0 = Variable(x0.long(), requires_grad=False)\n",
    "    pred, x_init = malconv(x0)\n",
    "    initial_prob = sigmoid(pred).cpu().data.numpy()[0][0]\n",
    "\n",
    "    if (cave_start+12288)>2000000 or initial_prob<0.5:\n",
    "        continue\n",
    "    \n",
    "    index_to_consider = list(range(cave_start, cave_start+12288))\n",
    "    counter +=1 \n",
    "    for i in range(50):\n",
    "        gradient_f = loss_function_gradient(malconv, x0, x_init, use_cuda=True)\n",
    "        x_init = optimization_solver(gradient_f, index_to_consider, x_init)\n",
    "        progress = sigmoid(malconv.embedd_and_forward(x_init)).cpu().data.numpy()[0][0]\n",
    "        if progress<0.5:\n",
    "            print(progress)\n",
    "            success+=1\n",
    "            break\n",
    "    # Data to append as a dictionary\n",
    "    new_data = {'val_batch_data': val_batch_data, 'length': length, 'init_prob': initial_prob, 'iteration': i, 'progress':progress}\n",
    "    new_data_df = pd.DataFrame([new_data])\n",
    "    # Use pd.concat to append the new row\n",
    "    df = pd.concat([df, new_data_df], ignore_index=True)\n",
    "\n",
    "print(f\"Success: {success}/{counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47293dd-58c1-4950-b4ad-61929a6887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "df = pd.DataFrame(columns=['val_batch_data', 'length', 'init_prob', 'iteration', 'progress'])\n",
    "\n",
    "# Helper functions: Assuming loss_function_gradient, optimization_solver, etc., are defined elsewhere.\n",
    "\n",
    "def process_batch(val_batch_data, malconv, use_gpu):\n",
    "    # This function processes a single batch and returns the results to append to the dataframe\n",
    "    perturbation_size=4096\n",
    "    exe_input = val_batch_data[0].cuda() if use_gpu else val_batch_data[0]\n",
    "    data = exe_input[0].cpu().numpy()\n",
    "    length = data[-3]\n",
    "    cave_start = data[-2]\n",
    "    cave_length = data[-1]\n",
    "    if cave_length==0:\n",
    "        return None\n",
    "    data = data[:length]\n",
    "    data = np.concatenate([data, np.random.randint(0, 256, 2000000 - length)])\n",
    "\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    x0 = torch.from_numpy(np.array([data])).long().cuda() if use_gpu else torch.from_numpy(np.array([data])).long()\n",
    "    x0 = Variable(x0.long(), requires_grad=False)\n",
    "    try:\n",
    "        pred, x_init = malconv(x0)\n",
    "        initial_prob = sigmoid(pred).cpu().data.numpy()[0][0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if (cave_start+perturbation_size) > 2000000 or initial_prob < 0.5:\n",
    "        return None  # Skip if conditions are not met\n",
    "\n",
    "    index_to_consider = list(range(cave_start, cave_start+perturbation_size))\n",
    "\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            gradient_f = loss_function_gradient(malconv, x0, x_init, use_cuda=True)\n",
    "            x_init = optimization_solver(gradient_f, index_to_consider, x_init)\n",
    "            progress = sigmoid(malconv.embedd_and_forward(x_init)).cpu().data.numpy()[0][0]\n",
    "        except:\n",
    "            break\n",
    "        if progress < 0.5:\n",
    "            print(progress)\n",
    "            break\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'val_batch_data': val_batch_data,\n",
    "        'length': length,\n",
    "        'init_prob': initial_prob,\n",
    "        'iteration': i,\n",
    "        'progress': progress\n",
    "    }\n",
    "\n",
    "def append_to_df(results):\n",
    "    # Append data to DataFrame (if result is not None)\n",
    "    if results:\n",
    "        new_data_df = pd.DataFrame([results])\n",
    "        return new_data_df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Assuming validloader and malconv are already defined\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Parallelize batch processing using concurrent.futures.ThreadPoolExecutor\n",
    "counter = 0\n",
    "success=0\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for val_batch_data in validloader:\n",
    "        futures.append(executor.submit(process_batch, val_batch_data, malconv, use_gpu))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        counter +=1\n",
    "        if counter%100 == 0:\n",
    "            print(f\" Completed :{counter}\")\n",
    "        result = future.result()\n",
    "        if result and result['progress'] < 0.5:\n",
    "            success += 1\n",
    "            print(f\" Success :{success}\")\n",
    "        # new_data_df = append_to_df(result)\n",
    "        # df = pd.concat([df, new_data_df], ignore_index=True)\n",
    "\n",
    "print(f\"Success: {success}/{counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa55cf-2369-4ede-a6f8-181ce7edcc77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
